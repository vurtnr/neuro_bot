#!/usr/bin/env python3
import sys
import os

venv_lib_path = "/home/ubuntu/neuro_bot_env/lib/python3.12/site-packages"
if os.path.exists(venv_lib_path) and venv_lib_path not in sys.path:
    sys.path.insert(0, venv_lib_path)
    print(f"âœ… å·²å¼ºåˆ¶åŠ è½½è™šæ‹Ÿç¯å¢ƒåº“: {venv_lib_path}")


import rclpy
from rclpy.node import Node
from rclpy.callback_groups import ReentrantCallbackGroup
from std_msgs.msg import String

# å¼•å…¥æˆ‘ä»¬åˆšæ‰å®šä¹‰çš„æ¥å£
from robot_interfaces.msg import AudioSpeech, RobotState
from robot_interfaces.srv import AskLLM

# å¼•å…¥ä¹‹å‰çš„ AI åº“
import sherpa_ncnn
import sherpa_onnx
import wave
import numpy as np
import soundfile as sf
import os
import subprocess
from openai import OpenAI
import threading
import time

# ================= é…ç½® (ä¸ä¹‹å‰ä¿æŒä¸€è‡´) =================
API_KEY = "sk-6e4321b5364d4883aa8224d49ca58ae1" # <--- âš ï¸ è®°å¾—å¡«ä½ çš„ Key
BASE_URL = "https://api.deepseek.com/v1"
LLM_MODEL = "deepseek-chat"
STT_MODEL_DIR = "/home/ubuntu/sherpa_model/sherpa-ncnn-streaming-zipformer-zh-14M-2023-02-23"
TTS_MODEL_DIR = "/home/ubuntu/sherpa_tts_model/vits-zh-aishell3"

class AudioNode(Node):
    def __init__(self):
        super().__init__('audio_node')
        self.get_logger().info("ğŸ¤ Audio Engine æ­£åœ¨å¯åŠ¨...")

        # 1. åˆå§‹åŒ– AI æ¨¡å‹ (STT / TTS / LLM)
        self.init_models()

        # 2. é€šä¿¡æ¥å£
        # [Pub] å‘Šè¯‰å¤§è„‘æˆ‘å¬åˆ°äº†ä»€ä¹ˆ
        self.speech_pub = self.create_publisher(AudioSpeech, '/speech/text', 10)
        
        # [Sub] å¬å¤§è„‘æŒ‡æŒ¥ï¼šè®©æˆ‘è¯´è¯
        self.tts_sub = self.create_subscription(String, '/mouth/say', self.speak_callback, 10)
        
        # [Service] å€Ÿå¤§è„‘ç®—åŠ›ï¼šLLM é—®ç­”
        self.llm_service = self.create_service(AskLLM, '/brain/ask_llm', self.handle_ask_llm)

        # 3. å¼€å¯å½•éŸ³çº¿ç¨‹ (ç®€åŒ–ç‰ˆï¼šå¾ªç¯æ£€æµ‹éŸ³é¢‘æ–‡ä»¶ï¼Œæœªæ¥å¯æ¥éº¦å…‹é£æµ)
        # è¿™é‡Œä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬æš‚æ—¶ä¸åšæ— é™å¾ªç¯å½•éŸ³ï¼Œè€Œæ˜¯ç­‰å¾…å¤–éƒ¨æŒ‡ä»¤æˆ–æ‰‹åŠ¨è§¦å‘
        # å®é™…éƒ¨ç½²æ—¶ï¼Œè¿™é‡Œä¼šæ˜¯ä¸€ä¸ª VAD å¾ªç¯
        self.get_logger().info("âœ… Audio Engine å°±ç»ªï¼ç­‰å¾… Rust å¤§è„‘æŒ‡ä»¤...")

    def init_models(self):
        # --- STT ---
        try:
            self.recognizer = sherpa_ncnn.Recognizer(
                tokens=f"{STT_MODEL_DIR}/tokens.txt",
                encoder_param=f"{STT_MODEL_DIR}/encoder_jit_trace-pnnx.ncnn.param",
                encoder_bin=f"{STT_MODEL_DIR}/encoder_jit_trace-pnnx.ncnn.bin",
                decoder_param=f"{STT_MODEL_DIR}/decoder_jit_trace-pnnx.ncnn.param",
                decoder_bin=f"{STT_MODEL_DIR}/decoder_jit_trace-pnnx.ncnn.bin",
                joiner_param=f"{STT_MODEL_DIR}/joiner_jit_trace-pnnx.ncnn.param",
                joiner_bin=f"{STT_MODEL_DIR}/joiner_jit_trace-pnnx.ncnn.bin",
                num_threads=4,
            )
        except Exception as e:
            self.get_logger().error(f"STT åŠ è½½å¤±è´¥: {e}")

        # --- TTS ---
        try:
            self.tts_engine = sherpa_onnx.OfflineTts(
                sherpa_onnx.OfflineTtsConfig(
                    model=sherpa_onnx.OfflineTtsModelConfig(
                        vits=sherpa_onnx.OfflineTtsVitsModelConfig(
                            model=f"{TTS_MODEL_DIR}/vits-aishell3.onnx",
                            lexicon=f"{TTS_MODEL_DIR}/lexicon.txt",
                            tokens=f"{TTS_MODEL_DIR}/tokens.txt",
                        ),
                        provider="cpu", num_threads=4, debug=False
                    )
                )
            )
        except Exception as e:
            self.get_logger().error(f"TTS åŠ è½½å¤±è´¥: {e}")

        # --- LLM ---
        self.client = OpenAI(api_key=API_KEY, base_url=BASE_URL)

    # === å›è°ƒå‡½æ•° ===

    def speak_callback(self, msg):
        """æ”¶åˆ° /mouth/say æ¶ˆæ¯æ—¶è§¦å‘"""
        text = msg.data
        self.get_logger().info(f"ğŸ‘„ æ­£åœ¨è¯´è¯: {text}")
        
        # ç®€å•çš„æ–‡æœ¬æ¸…æ´—
        safe_text = text.replace("NeuroBot", "çº½ç½—æ³¢ç‰¹").replace("AI", "äººå·¥æ™ºèƒ½")
        
        try:
            audio = self.tts_engine.generate(safe_text, sid=0, speed=1.1)
            sf.write("temp_speak.wav", audio.samples, audio.sample_rate)
            subprocess.run(["aplay", "-q", "temp_speak.wav"])
        except Exception as e:
            self.get_logger().error(f"TTS ç”Ÿæˆå¤±è´¥: {e}")

    def handle_ask_llm(self, request, response):
        """æ”¶åˆ° /brain/ask_llm è¯·æ±‚æ—¶è§¦å‘"""
        question = request.question
        self.get_logger().info(f"ğŸ§  å¤§è„‘è¯·æ±‚æ€è€ƒ: {question}")
        
        try:
            res = self.client.chat.completions.create(
                model=LLM_MODEL,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯NeuroBotã€‚è¯·ç”¨ç®€çŸ­ä¸­æ–‡å›ç­”ã€‚"},
                    {"role": "user", "content": question}
                ],
                stream=False
            )
            answer = res.choices[0].message.content
            response.answer = answer
            response.success = True
            self.get_logger().info(f"ğŸ’¡ æ€è€ƒå®Œæˆ: {answer}")
        except Exception as e:
            response.answer = f"æ€è€ƒå‡ºé”™: {str(e)}"
            response.success = False
            self.get_logger().error(f"LLM è°ƒç”¨å¤±è´¥: {e}")
        
        return response

def main(args=None):
    rclpy.init(args=args)
    node = AudioNode()
    rclpy.spin(node)
    node.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__':
    main()