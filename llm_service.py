#!/usr/bin/env python3
import os
import rclpy
from rclpy.node import Node
from openai import OpenAI

# å¯¼å…¥æœåŠ¡å®šä¹‰
from robot_interfaces.srv import AskLLM

# --- é…ç½®åŒºåŸŸ ---
# å¯ä»¥åœ¨è¿™é‡Œä¿®æ”¹æœºå™¨äººçš„åˆå§‹äººè®¾
SYSTEM_PROMPT = """
ä½ æ˜¯ä¸€ä¸ªè¿è¡Œåœ¨åµŒå…¥å¼è®¾å¤‡ä¸Šçš„æ™ºèƒ½æœºå™¨äººåŠ©æ‰‹ã€‚
è¯·ç”¨ç®€çŸ­ã€å£è¯­åŒ–ã€æœ‰äº›ä¿çš®çš„é£æ ¼å›ç­”ç”¨æˆ·çš„æé—®ã€‚
å› ä¸ºä½ æ˜¯ç”¨è¯­éŸ³æ’­æŠ¥çš„ï¼Œå›ç­”ä¸è¦å¤ªé•¿ï¼ˆæ§åˆ¶åœ¨50å­—ä»¥å†…ï¼‰ï¼Œä¸è¦ä½¿ç”¨å¤æ‚çš„Markdownæ ¼å¼ã€‚
"""
# ----------------

class LLMEngine(Node):
    def __init__(self):
        super().__init__('llm_engine')
        
        # 1. è·å– API Key (ä»ç¯å¢ƒå˜é‡)
        self.api_key = os.getenv("DEEPSEEK_API_KEY")
        if not self.api_key:
            self.get_logger().error("âŒ æœªæ‰¾åˆ° DEEPSEEK_API_KEY ç¯å¢ƒå˜é‡ï¼è¯· export DEEPSEEK_API_KEY='sk-...'")
            # è¿™é‡Œä¸é€€å‡ºï¼Œåªæ˜¯æŠ¥é”™ï¼Œæ–¹ä¾¿è°ƒè¯•
        
        # 2. åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯ (é€‚é… DeepSeek)
        # DeepSeek çš„ base_url æ˜¯ https://api.deepseek.com
        if self.api_key:
            self.client = OpenAI(api_key=self.api_key, base_url="https://api.deepseek.com/v1")
            self.get_logger().info("âœ… å·²è¿æ¥ DeepSeek API")
        else:
            self.client = None

        # 3. åˆ›å»ºæœåŠ¡
        self.srv = self.create_service(AskLLM, '/brain/ask_llm', self.handle_llm_request)
        self.get_logger().info("ğŸ¤– LLM Engine READY! Service: /brain/ask_llm")

    def handle_llm_request(self, request, response):
        question = request.question
        self.get_logger().info(f"ğŸ“¥ æ”¶åˆ°é—®é¢˜: \"{question}\"")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ Key
        if not self.client:
            response.answer = "æˆ‘çš„å¤§è„‘è¿˜æ²¡æœ‰é…ç½® API Keyï¼Œè¯·æ£€æŸ¥ç¯å¢ƒå˜é‡ã€‚"
            response.success = False
            return response

        try:
            self.get_logger().info("ğŸ¤” æ­£åœ¨è¯·æ±‚ DeepSeek...")
            
            # --- çœŸæ­£çš„ API è°ƒç”¨ ---
            completion = self.client.chat.completions.create(
                model="deepseek-chat",  # æˆ–è€… "deepseek-coder"
                messages=[
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": question}
                ],
                stream=False,
                temperature=0.7, # ç¨å¾®æœ‰åˆ›é€ åŠ›ä¸€ç‚¹
                max_tokens=100   # é™åˆ¶é•¿åº¦ï¼Œé˜²æ­¢è¯´å¤ªå¤šè¯
            )
            
            # æå–å›ç­”
            real_answer = completion.choices[0].message.content.strip()
            
            self.get_logger().info(f"ğŸ’¡ DeepSeek å›å¤: \"{real_answer}\"")
            
            response.answer = real_answer
            response.success = True
            
        except Exception as e:
            error_msg = f"API è°ƒç”¨å¤±è´¥: {str(e)}"
            self.get_logger().error(error_msg)
            response.answer = "æˆ‘å¥½åƒæ–­ç½‘äº†ï¼Œæ— æ³•è¿æ¥åˆ°äº‘ç«¯å¤§è„‘ã€‚"
            response.success = False
            
        return response

def main(args=None):
    rclpy.init(args=args)
    node = LLMEngine()
    try:
        rclpy.spin(node)
    except KeyboardInterrupt:
        pass
    finally:
        node.destroy_node()
        rclpy.shutdown()

if __name__ == '__main__':
    main()